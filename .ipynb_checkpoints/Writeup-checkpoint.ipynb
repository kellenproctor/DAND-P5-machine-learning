{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P5: Identify Fraud from Enron Email\n",
    "##### Udacity Data Analyst Nanodegree\n",
    "##### Kellen Proctor\n",
    "##### 29 June 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Exploration\n",
    "\n",
    "I began by exploring the features of data_dict, to get a full understanding of what I am dealing with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Summary\n",
    "\n",
    "The goal is simply to identify persons of interest (\"POI\") at Enron, ie those that were likely to have organized, perpetrated, and profited from the ongoing fraudulent activities at the firm. By the way, I watched the documentary, and being from California, what they did was just unconscionable. I have no idea how they were able to get access to the Power Grid, being able to shut off power plants and all, but it's kinda messed up that more people didn't get indicted. Regardless... The dataset has a wealth of financial and email related features. It would be difficult to identify POIs by hand. Instead, we can use machine learning algorithms to try to come up with a way to determine POIs programmatically.\n",
    "\n",
    "### Data Exploration\n",
    "There are 146 datapoints and 18 of those are POIs. The dataset contains 20 different features, split bewteen 14 financial and 6 email related features, for the people we are investigating. There's also a label - \"poi\" - that we can use for identifying persons of interest. Interestingly enough, only the features 'expenses', 'other', 'total_payments', 'total_stock_value', (and with 1 exception) 'restricted_stock', and 'salary' had valid data for all POIs. I may choose to use just these features in feature selection, but I'd like to keep all the features for now, and see what the results of using feature selection algorithms show.\n",
    "\n",
    "### Outliers\n",
    "I simply printed out a list of the keys of data_dict. Based on the outliers mini-project, and a couple forum entries, I found \"TOTAL\" and \"THE TRAVEL AGENCY IN THE PARK.\" Both were not POIs, from looking at their data, they either skewed the total data, or had none. So I removed both of these from the dataset. I chose to leave the remaining entries in data_dict, as they represent actual people. Further, the dataset is already very tiny, so I don't want to lose more information if I don't have to.\n",
    "\n",
    "After running a few scripts to analyze the remaining persons and features, I chose to also remove 'LOCKHARD EUGENE E', who basically had no data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Selection\n",
    "To start, I added all the features available, with the exception of emails.\n",
    "\n",
    "I created fractional features for payments, stock value, and email related features. This was based off of a forum post I read, but I figured it also may make certain features easier to compare between individuals, since some features like salary and bonus won't be so skewed by outlying individuals.\n",
    "\n",
    "I then removed 'loan_advances', 'director_fees', and 'restricted_stock_deferred', as well as their fractional equivalents, from the features list. These features had so little data overall, I figured they would contribute very little to the overall model. (Further explained below)\n",
    "\n",
    "After experimenting with various values of k for SelectKBest with a naive bayes classifier, I ended up using the following list of features ('salary', 'bonus_fraction', 'long_term_incentive', 'deferred_income', 'deferral_payments', 'other', 'expenses', 'total_payments', 'exercised_stock_options', 'restricted_stock', 'total_stock_value', 'from_this_person_to_poi', 'to_messages', 'from_poi_to_this_person', 'from_messages', 'shared_receipt_with_poi'), which would be trimmed down in an optimized GridSearchCV pipeline, before being fed to the Naive Bayes classifier I ended up using. The top 5 features being used in the naive bayes classifier, and their respective scores, are here:\n",
    "\n",
    "| Feature | Score | p-value|\n",
    "|---|---|---|\n",
    "|'total_payments'| 24.815079733218194| 1.8182048777865317e-06|\n",
    "|'restricted_stock'| 24.182898678566879| 2.4043152760436886e-06|\n",
    "|'salary'| 20.715596247559954| 1.1403848516905529e-05|\n",
    "|'long_term_incentive'| 11.458476579280369| 0.00092203670846723062|\n",
    "|'bonus_fraction'| 9.9221860131898225| 0.0019941812453537077|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a. Explanation of New Features\n",
    "\n",
    "I created a series of fractional features, with name and formula as below:\n",
    "\n",
    "##### Full created features list\n",
    "```\n",
    "fractional_features_list = [\n",
    "    # Email related fractional features\n",
    "    'to_poi_fraction',\n",
    "    'from_poi_fraction',\n",
    "\n",
    "    # Payment related fractional features\n",
    "    'salary_fraction',\n",
    "    'bonus_fraction',\n",
    "    'long_term_incentive_fraction',\n",
    "    'deferred_income_fraction',\n",
    "    'deferral_payments_fraction',\n",
    "    'other_fraction',\n",
    "    'expenses_fraction',\n",
    "\n",
    "    # Stock related fractional features\n",
    "    'exercised_stock_options_fraction',\n",
    "    'restricted_stock_fraction']\n",
    "```\n",
    "\n",
    "##### Email Ratios\n",
    "The ratio of messages sent to POIs and total messages sent\n",
    "```\n",
    "to_poi_fraction = from_this_person_to_poi / from_messages\n",
    "```\n",
    "The ratio of messages received from POIs and total messages received\n",
    "```\n",
    "from_poi_fraction = from_poi_to_this_person / to_messages\n",
    "```\n",
    "##### Financial Ratios\n",
    "In the same way, I created the same ratios by dividing the individual payment or stock features by total_payments and total_stock_value, respectively.\n",
    "\n",
    "Payment related ratios\n",
    "```\n",
    "salary_fraction = salary / total_payments\n",
    "bonus_fraction = bonus / total_payments\n",
    "long_term_incentive_fraction = long_term_incentive / total_payments\n",
    "deferred_income_fraction = deferred_income / total_payments\n",
    "deferral_payments_fraction = deferral_payments / total_payments\n",
    "other_fraction = other / total_payments\n",
    "expenses_fraction = expenses / total_payments\n",
    "```\n",
    "Stock related ratios\n",
    "```\n",
    "exercised_stock_options_fraction = exercised_stock_options / total_stok_value\n",
    "restricted_stock_fraction = restricted_stock / total_stok_value\n",
    "```\n",
    "\n",
    "My reasoning for creating the ratio of individual values to total values was to scale features so that they might be more comparable between individuals. There were a lot of individual features that were skewed in favor of some individuals that had large payments. Further, there was a pretty good mix between POIs and non-POIs with regards to those large features. I wanted to explore if the composition of the individual features to the total was more important that the feature itself. For example, I figured that perhaps POIs had a larger composition of their total payments related to bonus than otherwise, or they had larger percentages of restricted stock values.\n",
    "\n",
    "To determine the value of these new features, I scored the entire feature list (new and existing features), using an ANOVA f classifier with SelectKBest. I found that, aside from bonus_fraction, most of the fractional created features performed poorly. Because of this performance, I chose not to use the fractional features in my final algorithm, with the exception of bonus_fraction, which I used instead of bonus. I chose not to use salary_fraction, because salary performed slightly better, and I didn't want to use two similar features and risk adding collinearity into my model. This is also part of the reason I chose to exclude bonus. The full result of the SelectKBest scoring is listed here in this table.\n",
    "\n",
    "| Feature | Score | P-Value |\n",
    "|---------|-------|---------|\n",
    "|total_payments | 24.815079733218194 | 1.8182048777865317e-06|\n",
    "|restricted_stock_deferred | 24.182898678566879 | 2.4043152760436886e-06|\n",
    "|salary | 20.792252047181535 | 1.10129873239521e-05|\n",
    "|salary_fraction | 20.715596247559954 | 1.1403848516905529e-05|\n",
    "|poi | 18.289684043404513 | 3.4782737683651706e-05|\n",
    "|shared_receipt_with_poi | 16.409712548035792 | 8.3889533567042162e-05|\n",
    "|bonus_fraction | 13.850868417167648 | 0.00028512514965536027|\n",
    "|long_term_incentive | 11.458476579280369 | 0.00092203670846723062|\n",
    "|bonus | 9.9221860131898225 | 0.0019941812453537077|\n",
    "|exercised_stock_options | 9.2128106219771002 | 0.002862802957909168|\n",
    "|director_fees | 8.7727777300916756 | 0.0035893261725153044|\n",
    "|from_messages | 8.589420731682381 | 0.0039458026165322567|\n",
    "|deferral_payments | 7.1840556582887247 | 0.0082318509068451656|\n",
    "|other | 6.0941733106389453 | 0.014758199965371975|\n",
    "|deferral_payments_fraction | 5.3463405918745792 | 0.02221386974369996|\n",
    "|to_messages | 5.2434497133749582 | 0.023513867086669717|\n",
    "|loan_advances | 4.1874775069953749 | 0.042581747012345836|\n",
    "|to_poi_fraction | 3.1280917481567192 | 0.079116105663795441|\n",
    "|from_poi_fraction | 2.687417590844055 | 0.10337293194951264|\n",
    "|total_stock_value | 2.3826121082276739 | 0.12493365956927895|\n",
    "|expenses | 2.1263278020077054 | 0.14701111317392224|\n",
    "|from_this_person_to_poi | 1.6463411294420076 | 0.20156265029435688|\n",
    "|deferred_income_fraction | 1.3604336117316531 | 0.24543136205493971|\n",
    "|exercised_stock_options_fraction | 1.0901571696328429 | 0.29822315230244967|\n",
    "|loan_advances_fraction | 1.0684733217820632 | 0.30306019680461482|\n",
    "|restricted_stock_fraction | 0.76477268331683201 | 0.383326169421491|\n",
    "|deferred_income | 0.22461127473600989 | 0.63628164745867344|\n",
    "|expenses_fraction | 0.20806402247992767 | 0.6489908434639271|\n",
    "|from_poi_to_this_person | 0.16970094762175533 | 0.68100330042070167|\n",
    "|long_term_incentive_fraction | 0.15002858163937585 | 0.69909208283267188|\n",
    "|restricted_stock | 0.065499652909942141 | 0.79837865656298534|\n",
    "|director_fees_fraction | 0.04211676849806735 | 0.8376933284866297|\n",
    "|other_fraction | 0.017509612462176017 | 0.89491675396671311|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Algorithm\n",
    "\n",
    "I ended up using the Naive Bayes classifier (GaussianNB).\n",
    "\n",
    "I originally set up a script to run a classification report for Gaussian Naive Bayes, Decision Trees, Random Forest, and AdaBoost classifiers on the data split using train_test_split with a test size of 20%. The results of this are here:\n",
    "\n",
    "| Algorithm    | Precision | Recall | F1 Score | Support |\n",
    "|--------------|-----------|--------|----------|---------|\n",
    "|    GaussNB   |    0.92   |  0.90  |   0.91   |    29   |\n",
    "| DecisionTree |    0.86   |  0.79  |   0.82   |    29   |\n",
    "| RandomForest |    0.86   |  0.90  |   0.88   |    29   |\n",
    "|   Adaboost   |    0.86   |  0.83  |   0.84   |    29   |\n",
    "\n",
    "Based on these results, I initially decided to use the Random Forest algorithm, and proceeded to tune the parameters of that, which I will explain below. However, I was unable to achieve 0.3 recall on the tester.py script with any configuration. On a whim, I ran Naive Bayes, and lowe and behold, it worked just fine. So my final submission uses Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Algorithm Tuning\n",
    "\n",
    "Tuning an algorithm is the process of adjusting the parameters so as to optimize your algorithm to the dataset it is being run on. We are trying to get the algorithm to the sweetspot between high bias and high variance. If our algorithm is too biased to the dataset, then it isn't training well, which results in poor performance. If our algorithm has too much variance, then it we may have overfit the algorithm to the training data. This means it won't generalize well, making it a poor predictor moving forward. Either way, we end up with an algorithm that reacts poorly with real world data outside of our training set. We are trying to get it to a point where the algorithm generalizes well to new data, but isn't too simple. That is the goal of tuning.\n",
    "\n",
    "I tuned the RandomForest algorithm by running it through a GridSearchCV pipeline, and adjusting the parameters for n_estimators, max_features, and criterion, as well as the SearchKBest k-values. I also implemented three different scoring functions for GridSearchCV (accuracy_score, precision_score, and recall_score). I took the 3 best parameter settings for each scoring function, and proceeded with validation and evaluation as below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validation\n",
    "\n",
    "Validation is the process of running your trained algorithm on test dataset that is different from the data that you used to train the algorithm in the first place. Our goal is to give an estimate of the performance of the algorithm on an independent dataset and to check that we're not overfitting our algorithm, which would result in a poorly functioning algorithm. I validated my data by splitting the data set using train_test_split with a test size of 20%. I did this for both the RandomForest and Naive Bayes algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Usage of Evaluation Metrics\n",
    "\n",
    "The three evaluation metrics I primarily used are accuracy, precision, and recall. My average performace for those metrics with the RandomForest and Naive Bayes algorithms are the following:\n",
    "\n",
    "| Algorithm    | Accuracy | Precision | Recall |\n",
    "|--------------|----------|-----------|--------|\n",
    "| RandomForest | 0.85     | 0.35      | 0.15   |\n",
    "| NaiveBayes   | 0.84     | 0.40      | 0.31   |\n",
    "\n",
    "With regards to accuracy, on the most basic level, both algorithms were able to predict correctly, roughly 85% of the time, whether or not a person in the dataset was a Person of Interest (POI). However, accuracy just tells us if the prediction matches the actual label in the test dataset. We can further break that number down, and determine how many times we correctly predicted positive and negative values, and how many times our predictions were wrong positively or negatively. Precision tells us then how many times we were correct in predicting someone to be a POI, out of all positive predictions of people being POIs. With a precision rate of 0.40 for NaiveBayes and 0.35 for RandomForest, we were only good at positively identifying a POI at best 40% of the time.  As for Recall, we're trying to figure out how many times we predicted someone being a POI, and they actually were a POI. For recall, our best was .31 with the naive bayes algorithm. These results aren't actually that great. Overall, we're decent at predicting the outcome, but we're not that good at actually predicting the outcome correctly and consistently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "With more time to try and tune different algorithms, it might be possible to find a combination that improves our precision and recall, such that we can predict the fraudsters at Enron using their financial and email data. For now, our algorithms of choice just aren't that impressive.\n",
    "\n",
    "This has been a really fun project, and I'm glad I had the opportunity to work on it. I hope that my answers satisfy your requirements. Thank you very much for taking the time and energy to read and review my work! I hope you have a wonderful morning/day/evening!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources:\n",
    "\n",
    "\n",
    "Udacity Forums:\n",
    "https://discussions.udacity.com/t/how-to-start-the-final-project/177617/6\n",
    "\n",
    "https://discussions.udacity.com/t/project-fear-strugging-with-machine-learning-project/198529/3\n",
    "\n",
    "https://discussions.udacity.com/t/pickling-pandas-df/174753/2\n",
    "\n",
    "https://discussions.udacity.com/t/how-to-go-about-p5/259206/10\n",
    "\n",
    "https://discussions.udacity.com/t/do-we-just-remove-all-outliers/284364/4\n",
    "\n",
    "https://discussions.udacity.com/t/verify-final-project-approach/244607\n",
    "\n",
    "SKLearn Cheatsheet:\n",
    "http://scikit-learn.org/stable/_static/ml_map.png\n",
    "\n",
    "SKLearn Documentation (used a lot of this):\n",
    "http://scikit-learn.org/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary if needed\n",
    "\n",
    "__Salary__ - Reflects items such base salary, executive cash allowances, and benefits payments.\n",
    "\n",
    "__Bonus__ - Reflects annual cash incentives paid based upon company performance. Also may include other retention payments.\n",
    "\n",
    "__Long Term Incentive__ - Reflects long-term incentive cash payments from various long-term incentive programs designed to tie executive compensation to long-term success as measured against key performance drivers and business objectives over a multi-year period, generally 3 to 5 years.\n",
    "\n",
    "__Deferred Income__ - Reflects voluntary executive deferrals of salary, annual cash incentives, and long-term cash incentives as well as cash fees deferred by non-employee directors under a deferred compensation arrangement. May also reflect deferrals under a stock option or phantom stock unit in lieu of cash arrangement.\n",
    "\n",
    "__Deferral Payments__ - Reflects distributions from a deferred compensation arrangement due to termination of employment or due to in-service withdrawals as per plan provisions.\n",
    "\n",
    "__Loan Advances__ - Reflects total amount of loan advances, excluding repayments, provided by the Debtor in return for a promise of repayment. In certain instances, the terms of the promissory notes allow for the option to repay with stock of the company.\n",
    "\n",
    "__Other__ - Reflects items such as payments for severance, consulting services, relocation costs, tax advances and allowances for employees on international assignment (i.e. housing allowances, cost of living allowances, payments under Enronâ€™s Tax Equalization Program, etc.). May also include payments provided with respect to employment agreements, as well as imputed income amounts for such things as use of corporate aircraft.\n",
    "\n",
    "__Expenses__ - Reflects reimbursements of business expenses. May include fees paid for consulting services.\n",
    "\n",
    "__Director Fees__ - Reflects cash payments and/or value of stock grants made in lieu of cash payments to non-employee directors.\n",
    "\n",
    "__Exercised Stock Options__ - Reflects amounts from exercised stock options which equal the market value in excess of the exercise price on the date the options were exercised either through cashless (same-day sale), stock swap or cash exercises. The reflected gain may differ from that realized by the insider due to fluctuations in the market price and the timing of any subsequent sale of the securities.\n",
    "\n",
    "__Restricted Stock__ - Reflects the gross fair market value of shares and accrued dividends (and/or phantom units and dividend equivalents) on the date of release due to lapse of vesting periods, regardless of whether deferred.\n",
    "\n",
    "__Restricted Stock Deferred__ - Reflects value of restricted stock voluntarily deferred prior to release under a deferred compensation arrangement."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dandy]",
   "language": "python",
   "name": "conda-env-dandy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
